{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice de la práctica\n",
    "\n",
    "- [1.Creación de datos](#1.Creación-de-datos)\n",
    "- [Regresión logística](#Regresión-logística)\n",
    "- [Neural Networks](#Neural-Networks)\n",
    "- [Detección de Spam con Naïve Bayes (versión Multinomial)](Detección-de-Spam-con-Naïve-Bayes-(versión-Multinomial))\n",
    "- [Tarea extra optativa](Tarea-extra-optativa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import skimage\n",
    "\n",
    "\n",
    "#Librerias contornos \n",
    "import numpy as np\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.filters import prewitt_h,prewitt_v\n",
    "from skimage import data, io\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.filters import threshold_otsu \n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris #dataset de prueba\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.optimize import minimize\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "#para datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Creación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a crear la matriz X, que es la que contiene las imágenes (se crea un dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$devolverDiccionarioEtiquetas$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG\n",
    "def hogMaker(images00000_mod):\n",
    "    fd, hog_image = hog(images00000_mod, orientations=8, pixels_per_cell=(16, 16),\n",
    "                        cells_per_block=(1, 1), visualize=True)\n",
    "\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "#     ax1.axis('off')\n",
    "#     ax1.imshow(images00000_mod, cmap=plt.cm.gray)\n",
    "#     ax1.set_title('Input image')\n",
    "\n",
    "#     # Rescale histogram for better display\n",
    "#     hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "#     ax2.axis('off')\n",
    "    #ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "    #ax2.set_title('Histogram of Oriented Gradients')\n",
    "    #plt.show()\n",
    "    return fd\n",
    "\n",
    "\n",
    "# $Sift$\n",
    "#devuelve descriptor de Sift\n",
    "def siftMaker(images00000_mod):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(images00000_mod,None)    \n",
    "    #quizas falta un tercer parametro:  imagen original\n",
    "    img=cv2.drawKeypoints(images00000_mod,kp, images00000_mod, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    #cv2.imwrite('sift_keypoints.jpg',img)\n",
    "    return des\n",
    "\n",
    "\n",
    "# $Surf$\n",
    "def surfmaker(images00000_mod):\n",
    "    surf = cv2.xfeatures2d.SURF_create(400)\n",
    "    kp, des = surf.detectAndCompute(images00000_mod,None)\n",
    "    return des\n",
    "\n",
    "# $Fast$ + $Brief$\n",
    "def fastbrief_Maker(images00000_mod):\n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "    # find and draw the keypoints\n",
    "    kp2 = fast.detect(images00000_mod,None)\n",
    "    img2 = cv2.drawKeypoints(images00000_mod, kp2, None, color=(255,0,0))\n",
    "\n",
    "    # Disable nonmaxSuppression\n",
    "    fast.setNonmaxSuppression(0)\n",
    "    kp3 = fast.detect(img2,None)\n",
    "\n",
    "    img3 = cv2.drawKeypoints(img2, kp3, None, color=(255,0,0))\n",
    "    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "    # Compute descriptors\n",
    "    kp, des = brief.compute(img3, kp3) #fast nonsup\n",
    "    return des\n",
    "\n",
    "\n",
    "# $ORB$\n",
    "def orbMaker(images00000_mod):\n",
    "    # Initiate ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # find the keypoints with ORB\n",
    "    kp = orb.detect(images00000_mod,None)\n",
    "\n",
    "    # compute the descriptors with ORB\n",
    "    kp, des = orb.compute(images00000_mod, kp)\n",
    "\n",
    "    # draw only keypoints location,not size and orientation\n",
    "#     img2 = cv2.drawKeypoints(images00000_mod, kp, None, color=(0,255,0), flags=0)\n",
    "    #plt.imshow(img2), plt.show()\n",
    "    return des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Descripcion: diccionario con los posibles valores de las imagenes, en relacion con el nombre de las carpetas.\n",
    "Entrada: -\n",
    "Salida: diccionario cuyas claves son el nombre de las carpetas, y los valores son el tipo de signals que contiene.\n",
    "'''\n",
    "\n",
    "\n",
    "def devolverDiccionarioEtiquetas():\n",
    "    \n",
    "    diccionarioValores = {}\n",
    "    diccionarioValores['00000'] = 'VelocidadMaxima20'\n",
    "    diccionarioValores['00001'] = 'VelocidadMaxima30'\n",
    "    diccionarioValores['00002'] = 'VelocidadMaxima50'\n",
    "    diccionarioValores['00003'] = 'VelocidadMaxima60'\n",
    "    diccionarioValores['00004'] = 'VelocidadMaxima70'\n",
    "    diccionarioValores['00005'] = 'VelocidadMaxima80'\n",
    "    diccionarioValores['00006'] = 'FinVelocidadMaxima80'\n",
    "    diccionarioValores['00007'] = 'VelocidadMaxima100'\n",
    "    diccionarioValores['00008'] = 'VelocidadMaxima120'\n",
    "    diccionarioValores['00009'] = 'AdelantamientoProhibido'\n",
    "    diccionarioValores['00010'] = 'AdelantamientoProhibidoParaCamiones'\n",
    "\n",
    "    diccionarioValores['00011'] = 'InterseccionConPrioridad'\n",
    "    diccionarioValores['00012'] = 'CalzadaConPrioridad'\n",
    "    diccionarioValores['00013'] = 'CedaElPaso'\n",
    "    diccionarioValores['00014'] = 'DetencionObligatoria'\n",
    "    diccionarioValores['00015'] = 'CirculacionProhibida'\n",
    "    diccionarioValores['00016'] = 'EntradaProhibidaAVehiculosMercancias'\n",
    "    diccionarioValores['00017'] = 'EntradaProhibida'\n",
    "    diccionarioValores['00018'] = 'OtrosPeligros'\n",
    "    diccionarioValores['00019'] = 'CurvaPeligrosaHaciaLaIzquierda'\n",
    "    diccionarioValores['00020'] = 'CurvaPeligrosaHaciaLaDerecha'\n",
    "\n",
    "    diccionarioValores['00021'] = 'CurvasPeligrosasHaciaLaIzquierda'\n",
    "    diccionarioValores['00022'] = 'PerfilIrregular'\n",
    "    diccionarioValores['00023'] = 'PavimentoDeslizante'\n",
    "    diccionarioValores['00024'] = 'EstrechamientoCalzadaPorDerecha'\n",
    "    diccionarioValores['00025'] = 'Obras'\n",
    "    diccionarioValores['00026'] = 'Semaforos'\n",
    "    diccionarioValores['00027'] = 'PasoDePeatones'\n",
    "    diccionarioValores['00028'] = 'Kids'\n",
    "    diccionarioValores['00029'] = 'EntradaProhibidasCiclos'\n",
    "    diccionarioValores['00030'] = 'PavimentoDeslizanteNieveHielo'\n",
    "\n",
    "    diccionarioValores['00031'] = 'PasoDeAnimalesEnLibertad'\n",
    "    diccionarioValores['00032'] = 'FinDeProhibiciones'\n",
    "    diccionarioValores['00033'] = 'SentidoObligatorioDerecha'\n",
    "    diccionarioValores['00034'] = 'SentidoObligatorioIzquierda'\n",
    "    diccionarioValores['00035'] = 'SentidoObligatorio'\n",
    "    diccionarioValores['00036'] = 'DirPermitidasRectoYDerecha'\n",
    "    diccionarioValores['00037'] = 'DirPermitidasRectoEIzquierda'\n",
    "    diccionarioValores['00038'] = 'PasoObligatorioDerecha'\n",
    "    diccionarioValores['00039'] = 'PasoObligatorioizquierda'\n",
    "    diccionarioValores['00040'] = 'InterseccionSentidoObligatorioGiratorio'\n",
    "\n",
    "\n",
    "    diccionarioValores['00041'] = 'FinProhibicionAdelantamiento'\n",
    "    diccionarioValores['00042'] = 'FinProhibicionAdelantamientoCamiones'\n",
    "    \n",
    "    return diccionarioValores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos ahora el código...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d =  devolverDiccionarioEtiquetas()\n",
    "\n",
    "for k,v in d.items():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFICAR!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearDiccionarioDatos():\n",
    "    trafic_signals = {}\n",
    "    #para cada tipo de señal creamos una lista con sus ejemplos, y se la asignamos a su etiqueta de nombre de carpeta\n",
    "    for i in range(0,43):\n",
    "        if i<10:\n",
    "            folder = 'Dataset_traffic_sign/0000'+str(i)+'/*.png'\n",
    "            tipo = '0000'+str(i)\n",
    "        else:\n",
    "            folder = 'Dataset_traffic_sign/000'+str(i)+'/*.png'\n",
    "            tipo = '000'+str(i)\n",
    "        \n",
    "        images = [cv2.imread(file,0) for file in glob.glob(folder)]\n",
    "        images_mod = [] \n",
    "        for im in images:\n",
    "            #se aplica el resize\n",
    "            images_mod.append(cv2.resize(im, (100, 100), interpolation = cv2.INTER_NEAREST))\n",
    "\n",
    "        trafic_signals[tipo] = images_mod\n",
    "    return trafic_signals\n",
    "\n",
    "# Creamos nuestra matriz de datos\n",
    "def creaMatrizCaracteristicas(trafic_signals, signals_types, carac_type):\n",
    "    Xtrain = 0\n",
    "    Ytrain = 0\n",
    "    Xtest = 0\n",
    "    Ytest = 0\n",
    "    \n",
    "    for signal,images in trafic_signals.items():\n",
    "        x = []\n",
    "        y = []\n",
    "        \n",
    "        x = obtenerDesCaracteristicas(np.array(images), carac_type)\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            y.append(signals_types[signal])\n",
    "            \n",
    "        y = np.array(y).reshape(-1,1)\n",
    "        \n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(x, y,test_size=0.4, stratify=y,random_state=1)\n",
    "        \n",
    "        if signal=='00000':\n",
    "            Xtrain = xtrain\n",
    "            Ytrain = ytrain\n",
    "            Xtest = xtest\n",
    "            Ytest = ytest\n",
    "        else:\n",
    "            Xtrain = np.vstack((Xtrain,xtrain))\n",
    "            Ytrain =  np.vstack((Ytrain,ytrain))\n",
    "            Xtest = np.vstack((Xtest,xtest))\n",
    "            Ytest = np.vstack((Ytest,ytest))\n",
    "    \n",
    "    return Xtrain,Ytrain,Xtest,Ytest\n",
    "\n",
    "# Pasamos una np array de imagenes y vamos sacando sus descriptores 1 a 1\n",
    "def obtenerDesCaracteristicas(images_o, carac_type):\n",
    "    des = None\n",
    "    \n",
    "    for i in range(images_o.shape[0]):\n",
    "        if carac_type=='sift':\n",
    "            d = siftMaker(images_o[i])\n",
    "        elif carac_type=='surf':\n",
    "            d = surfmaker(images_o[i])\n",
    "        elif carac_type=='fast_brief':\n",
    "            d = fastbrief_Maker(images_o[i])\n",
    "        elif carac_type=='orb':\n",
    "            d = orbMaker(images_o[i])\n",
    "        elif carac_type=='hog':\n",
    "            d = hogMaker(images_o[i])\n",
    "        else:\n",
    "            try:\n",
    "                raise ValueError\n",
    "            except ValueError:\n",
    "                print(\"valor incorrecto\")\n",
    "                \n",
    "        if d is not None:\n",
    "            if des is None:\n",
    "                des = d\n",
    "            else:\n",
    "                des = np.vstack((des,d))\n",
    "    return des"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUEVO: cogemos las imagenes y las separamoms en train y test, y decimos a que clase pertenecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separaTrainTest(trafic_signals,signals_types):\n",
    "    imagenesTrain = None\n",
    "    imagenesTest = None\n",
    "    ytrain = None\n",
    "    ytest = None\n",
    "    \n",
    "    for signal,images in trafic_signals.items():\n",
    "        \n",
    "        aux_y_train = []\n",
    "        aux_y_test = []\n",
    "        \n",
    "        np_im = np.array(images)\n",
    "        n_train = int(0.6*np_im.shape[0])\n",
    "        n_test = np_im.shape[0]-n_train\n",
    "        \n",
    "        aux_train = np_im[:n_train]\n",
    "        aux_test = np_im[n_train:]\n",
    "        \n",
    "        for i in range(n_train):\n",
    "            aux_y_train.append(signals_types[signal])\n",
    "        for i in range(n_test):\n",
    "            aux_y_test.append(signals_types[signal])\n",
    "        \n",
    "        aux_y_train = np.array(aux_y_train).reshape(-1,1)\n",
    "        aux_y_test = np.array(aux_y_test).reshape(-1,1)\n",
    "        \n",
    "        if imagenesTrain is None:\n",
    "            imagenesTrain = aux_train\n",
    "            ytrain = aux_y_train\n",
    "            imagenesTest = aux_test\n",
    "            ytest = aux_y_test\n",
    "        else:\n",
    "            imagenesTrain = np.concatenate((imagenesTrain,aux_train),axis=0)\n",
    "            ytrain =  np.vstack((ytrain,aux_y_train))\n",
    "            imagenesTest = np.concatenate((imagenesTest,aux_test),axis=0)\n",
    "            ytest = np.vstack((ytest,aux_y_test))\n",
    "        \n",
    "    return imagenesTrain,imagenesTest,ytrain,ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUEVO: creamos las caracteristicas con los datos que nos den y sus tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creaMatrizCaracteristicas(imagenes,carac_type, y=None ):\n",
    "    X = None\n",
    "    if y is not None:\n",
    "        y_nueva = []\n",
    "        \n",
    "    for i in range(imagenes.shape[0]):\n",
    "        if carac_type=='sift':\n",
    "            d = siftMaker(imagenes[i])\n",
    "        elif carac_type=='surf':\n",
    "            d = surfmaker(imagenes[i])\n",
    "        elif carac_type=='fast_brief':\n",
    "            d = fastbrief_Maker(imagenes[i])\n",
    "        elif carac_type=='orb':\n",
    "            d = orbMaker(imagenes[i])\n",
    "        elif carac_type=='hog':\n",
    "            d = hogMaker(imagenes[i])\n",
    "        else:\n",
    "            try:\n",
    "                raise ValueError\n",
    "            except ValueError:\n",
    "                print(\"valor incorrecto\")\n",
    "        \n",
    "        if d is not None:\n",
    "#             print(d.shape)\n",
    "            if y is not None:\n",
    "                if y[i] == 'FinDeProhibiciones':\n",
    "                    print(y[i])\n",
    "                y_nueva.append(y[i])\n",
    "            if X is None:\n",
    "                X = d\n",
    "            else:\n",
    "                X = np.vstack((X,d))\n",
    "    if y is not None:\n",
    "        return X, np.array(y_nueva)\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafic_signals = crearDiccionarioDatos()\n",
    "signals_types = devolverDiccionarioEtiquetas()\n",
    "carac_type = 'hog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Xtrain,Ytrain,Xtest,Ytest = creaMatrizCaracteristicas(trafic_signals, signals_types, carac_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenesTrain,imagenesTest,ytrain,ytest = separaTrainTest(trafic_signals,signals_types)\n",
    "Xtrain, Ytrain = creaMatrizCaracteristicas(imagenesTrain,carac_type,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenesTrain.shape,imagenesTest.shape,ytrain.shape,ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape, Ytrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Aplicación de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se implementarán dos modelos de ML como son regresión logísitica y redes neuronales para dar solución al problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos hemos inclinado por RL en detrimento de SVM pues el número de características es bastante superior al número de ejemplos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformarY(y, signal):\n",
    "    aux = np.copy(y)\n",
    "    Y = np.zeros(y.shape[0]).reshape(-1,1)\n",
    "    \n",
    "    Y[aux != np.array([signal])] = 0\n",
    "    Y[aux == np.array([signal])] = 1\n",
    "    \n",
    "    return Y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(Ytrain == 'FinDeProhibiciones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicctionsTrain = []\n",
    "predicctionsTest = []\n",
    "final_signals = []\n",
    "\n",
    "for k,s in signals_types.items():\n",
    "    Ytrain_T = transformarY(Ytrain,s)\n",
    "#     Ytest_T = transformarY(Ytest,s)\n",
    "    print(s)\n",
    "    clf = LogisticRegression(random_state=0,solver='lbfgs',max_iter=10000).fit(Xtrain, Ytrain_T)\n",
    "    prediccionTrain = clf.predict(Xtrain)\n",
    "#     prediccionTest = clf.predict(Xtest)\n",
    "    scoreTrain = clf.score(Xtrain, Ytrain_T)\n",
    "#     scoreTest = clf.score(Xtest, Ytest_T)\n",
    "    \n",
    "#     print(f\"La precision del clasificador en TRAIN: {np.round(scoreTrain,2)} en TEST: {scoreTest} para {s}\")\n",
    "    \n",
    "    predicctionsTrain.append(np.round(scoreTrain,2))\n",
    "#     predicctionsTest.append(np.round(scoreTest,2))\n",
    "    final_signals.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = np.arange(len(final_signals))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(sig,predicctionsTrain,label='Resultados para Train')\n",
    "# plt.plot(sig,predicctionsTest,label='Resultados para Test')\n",
    "plt.xlabel('signals')\n",
    "plt.ylabel('precision')\n",
    "plt.grid()\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFICAR PARA TEST!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSignalsLogPredicction(Xtrain,Ytrain,signals_types,carac_type):\n",
    "        \n",
    "    predicctionsTrain = []\n",
    "    final_signals = []\n",
    "    modelos = []\n",
    "\n",
    "    for k,s in signals_types.items():\n",
    "        Ytrain_T = transformarY(Ytrain,s)\n",
    "        if not(np.any(Ytrain_T==1)):\n",
    "            predicctionsTrain.append(0)\n",
    "            final_signals.append(s)\n",
    "            modelos.append(None)\n",
    "        else:\n",
    "            clf = LogisticRegression(random_state=0,solver='lbfgs',max_iter=10000).fit(Xtrain, Ytrain_T)\n",
    "            prediccionTrain = clf.predict(Xtrain)\n",
    "    #         print(prediccionTrain.shape)\n",
    "            scoreTrain = clf.score(Xtrain, Ytrain_T)\n",
    "\n",
    "    #         print(f\"La precision del clasificador en TRAIN: {np.round(scoreTrain,2)} para {s}\")\n",
    "\n",
    "            predicctionsTrain.append(np.round(scoreTrain,2))\n",
    "            final_signals.append(s)\n",
    "            modelos.append(clf)\n",
    "    \n",
    "#     sig = np.arange(len(final_signals))\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     plt.ylim(0.8, 1)\n",
    "#     plt.plot(sig,predicctionsTrain,label='Resultados para Train')\n",
    "#     plt.title(carac_type)\n",
    "#     plt.xlabel('signals')\n",
    "#     plt.ylabel('precision')\n",
    "#     plt.grid()\n",
    "#     plt.legend(loc=4)\n",
    "#     plt.show()\n",
    "    \n",
    "    predMediaTrain = sum(predicctionsTrain)/len(predicctionsTrain)\n",
    "    \n",
    "    print(f'Prediccion media TRAIN: {predMediaTrain}')\n",
    "    \n",
    "    return modelos,predMediaTrain,np.array(predicctionsTrain)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deNumeroASignal(num):\n",
    "    if num<10:\n",
    "        tipo = '0000'+str(num)\n",
    "    else:\n",
    "        tipo = '000'+str(num)\n",
    "    return tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeDescriptors = ['hog']#['sift','surf','fast_brief','orb','hog']\n",
    "trafic_signals = crearDiccionarioDatos()\n",
    "signals_types = devolverDiccionarioEtiquetas()\n",
    "bestypeDes = None\n",
    "Modelos = None\n",
    "bestXtest = None\n",
    "bestYtrain = None\n",
    "bestPrediccionTrain = 0\n",
    "prediccionesPorSignal = None\n",
    "\n",
    "for carac_type in typeDescriptors:\n",
    "    \n",
    "    Xtrain,Ytrain = creaMatrizCaracteristicas(imagenesTrain,carac_type,ytrain)\n",
    "    \n",
    "    modelos, prediccionTrain, predicctionsTrain = getSignalsLogPredicction(Xtrain,Ytrain,signals_types,carac_type)\n",
    "    \n",
    "    if bestPrediccionTrain < prediccionTrain:\n",
    "        Modelos = modelos\n",
    "        bestypeDes = carac_type\n",
    "        prediccionesPorSignal = predicctionsTrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 61\n",
    "cv2.imshow('ventana', imagenesTrain[n])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "y_imagen = np.zeros(len(Modelos))\n",
    "\n",
    "#     transformar imagen en des\n",
    "d = creaMatrizCaracteristicas(np.array([imagenesTrain[n]]), bestypeDes)\n",
    "print(d.shape)\n",
    "d = np.array(d).reshape(1,-1)\n",
    "\n",
    "#     predecir des\n",
    "ii = 0\n",
    "for m in Modelos:\n",
    "    if m is not None:\n",
    "        y_imagen[ii] = m.predict(d)\n",
    "        print(y_imagen[ii])\n",
    "        ii+=1\n",
    "        \n",
    "#     sacar resultado\n",
    "pred = np.argwhere(y_imagen==1)\n",
    "print(pred)\n",
    "if pred.size>1:\n",
    "    a = deNumeroASignal(pred[np.argmax(prediccionesPorSignal[pred])])\n",
    "    print('----------')\n",
    "    print(a)\n",
    "    print(signals_types[a])\n",
    "elif pred.size == 1:\n",
    "    a = deNumeroASignal(pred[0][0])\n",
    "    print(signals_types[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(43):\n",
    "    if Modelos[i] is not None:\n",
    "        print(np.any(Modelos[i].predict(Xtrain)==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# probar con el conjunto de test\n",
    "print(bestypeDes)\n",
    "ytest = []\n",
    "\n",
    "for i in range(Xtest.shape[0]):\n",
    "\n",
    "    y_aux = np.zeros(len(Modelos))\n",
    "    z = 0\n",
    "    \n",
    "    for mod in Modelos:\n",
    "        y_aux[z] = mod.predict(Xtest[i].reshape(1,-1))\n",
    "        z+=1\n",
    "        \n",
    "    pred = np.argwhere(y_aux==1)\n",
    "    \n",
    "    if pred.size>1:\n",
    "        a = deNumeroASignal(pred[np.argmax(prediccionesPorSignal[pred])])\n",
    "        print('----------')\n",
    "        print(a)\n",
    "        ytest.append(signals_types[a])\n",
    "    else:\n",
    "        a = deNumeroASignal(pred)\n",
    "        print('++++++++++')\n",
    "        print(a)\n",
    "        b = signals_types[a]\n",
    "        print('ttttttttttttttt')\n",
    "        print(b)\n",
    "        ytest.append(b)\n",
    "ytest = np.array(ytest).reshape(-1,1)\n",
    "print(bestYtest.shape,ytest.shape)\n",
    "ytest==bestYtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecirImagen(imagen, Mod,prediccionesPorSignal,carac_type):\n",
    "    y_imagen = np.zeros(len(Mod))\n",
    "    \n",
    "#     transformar imagen en des\n",
    "    d = creaMatrizCaracteristicas(np.array([imagen]), carac_type)\n",
    "    print(d.shape)\n",
    "    d = np.array(d)\n",
    "    y_d = np.zeros(d.shape[0])\n",
    "#     predecir des\n",
    "    ii = 0\n",
    "    for m in Mod:\n",
    "        if m is not None:\n",
    "            for i in range(d.shape[0]):\n",
    "                y_d[i] = m.predict(d[i].reshape(1,-1))\n",
    "            print(y_d)\n",
    "            decision = np.where(y_d==1)[0].size\n",
    "            if decision >= int(y_d.size/2):\n",
    "                y_imagen[ii] = 1\n",
    "            ii+=1\n",
    "#     sacar resultado\n",
    "    pred = np.argwhere(y_imagen==1)\n",
    "    \n",
    "    if pred.size>1:\n",
    "        a = deNumeroASignal(pred[np.argmax(prediccionesPorSignal[pred])])\n",
    "        print('----------')\n",
    "        print(a)\n",
    "        return signals_types[a]\n",
    "    elif pred.size == 1:\n",
    "        a = deNumeroASignal(pred)\n",
    "        return signals_types[a]\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASAR POR AHORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "# Aplica el fit_transform de scikit a y para obtener el nuevo y_onehot\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1,1))\n",
    "\n",
    "#esto quiere decir que hay 'n' posibles clases. Se pondra un 1 en la clase que participa, y 0 en la clase que no participa.\n",
    "#La participacion va por  'registros'\n",
    "#Para mas info: https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n",
    "y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, theta1, theta2):\n",
    "    m = X.shape[0]\n",
    "    bias = np.ones(m).reshape(-1,1)\n",
    "    \n",
    "    # Añadimos la columna de unos a X para obtener a1\n",
    "    a1 = np.hstack((bias,X))\n",
    "    \n",
    "    # Calculamos z2 -> ten en cuenta que a1 tiene tantas filas como ejemplos y columnas como atributos + 1\n",
    "    # Por otro lado, theta1 tiene tantas filas como neuronas en la capa oculta y columnas como atributos + 1\n",
    "    z2 = a1*theta1.T\n",
    "    \n",
    "    # Añadimos la columna de unos a la sigmoide de z2 (que es a2) para obtener el a2 definitivo\n",
    "    a2 = np.hstack((np.ones(z2.shape[0]).reshape(-1,1),sigmoid(z2)))\n",
    "    \n",
    "    # Calculamos z3\n",
    "    z3 = a2*theta2.T\n",
    "    \n",
    "    # Obtenemos la salida final en h\n",
    "    h = sigmoid(z3)\n",
    "    \n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(params, input_size, hidden_size, num_labels, X, y):\n",
    "    m = X.shape[0]\n",
    "    # Utilizamos las matrices de numpy por facilidad: * es el producto de matrices y np.multiply elemento por elemento\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # desempaquetamos las matrices con los parámetros para cada capa\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    # Ejecutamos las propagación hacia adelante para obtener las salidas para cada ejemplo\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # Calculamos el coste\n",
    "    J = (1/m)*(np.sum(np.multiply((-y),np.log(h))-np.multiply((1-y),np.log(1-h))))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial\n",
    "input_size = X.shape[1] #numero de caracteristicas de X\n",
    "hidden_size = 25\n",
    "num_labels = y.size\n",
    "np.random.seed(123456789)\n",
    "\n",
    "# Inicializamos los parámetros de la red aleatoriamente\n",
    "# El tamaño del array es el tamaño de las dos matrices de pesos concatenadas\n",
    "params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)) - 0.5) * 0.25\n",
    "\n",
    "# Podemos desempaquetar los parámetros que acabamos de inicializar igual que lo hacemos en la función de coste\n",
    "theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "# Veamos si los tamaños de las matrices theta1 y theta2 son correctos\n",
    "theta1.shape, theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg = 1.0 #Parametro regularizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costReg(params, input_size, hidden_size, num_labels, X, y, lambda_reg):\n",
    "    m = X.shape[0]\n",
    "    # Utilizamos las matrices de numpy por facilidad: * es el producto de matrices y np.multiply elemento por elemento\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # desempaquetamos las matrices con los parámetros para cada capa, obtener theta1 y theta2\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    # Ejecutamos las propagación hacia adelante para obtener las salidas para cada ejemplo\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # Calculamos el coste\n",
    "    J = (1/m)*(np.sum(np.multiply((-y),np.log(h))-np.multiply((1-y),np.log(1-h))))\n",
    "    \n",
    "    # Es decir, la suma de los parámetros al cuadrado sin considerar la primera columna en ninguna de las dos matrices de parámetros\n",
    "    J += (lambda_reg/(2*m))*(np.sum(np.sum(np.multiply(theta1,theta1))+np.sum(np.multiply(theta2,theta2))))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    g = sigmoid(z)\n",
    "    return np.multiply(g,(1-g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropReg(params, input_size, hidden_size, num_labels, X, y, lambda_reg):\n",
    "    ###################################################################\n",
    "    # Copiar aquí el código de la función de coste con regularización #\n",
    "    ###################################################################\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    # Utilizamos las matrices de numpy por facilidad: * es el producto de matrices y np.multiply elemento por elemento\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # desempaquetamos las matrices con los parámetros para cada capa, obtener theta1 y theta2\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    # Ejecutamos las propagación hacia adelante para obtener las salidas para cada ejemplo\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # Calculamos el coste\n",
    "    J = (1/m)*(np.sum(np.multiply((-y),np.log(h))-np.multiply((1-y),np.log(1-h))))\n",
    "    \n",
    "    # Es decir, la suma de los parámetros al cuadrado sin considerar la primera columna en ninguna de las dos matrices de parámetros\n",
    "    J += (lambda_reg/(2*m))*(np.sum(np.sum(np.multiply(theta1,theta1))+np.sum(np.multiply(theta2,theta2))))\n",
    "    \n",
    "     ############################\n",
    "    # Comienza Backpropagation #\n",
    "    ############################\n",
    "    # Inicializamos los acumuladores delta1  y delta2 a ceros, con las dismensiones de los theta1 y theta2\n",
    "    # tendrán dimensiones (25, 401) y (10, 26), respectivamente\n",
    "    delta1 = np.zeros(theta1.shape)\n",
    "    delta2 = np.zeros(theta2.shape)\n",
    "    \n",
    "    # Aunque podríamos vectorizarlo vamos a hacerlo para cada ejemplo\n",
    "    for t in range(m):\n",
    "        # Obtenemos lo que necesitamos del ejemplo (cálculos obtenidos en la propagación hacia adelante)\n",
    "        # Para usar las fórmulas tal y como aparecen, vamos a coger todos los vectores en forma columna (resahpe(-1,1))\n",
    "        a1t = a1[t,:].reshape(-1, 1)  # (401, 1)\n",
    "        z2t = z2[t,:].reshape(-1, 1)  # (25, 1)\n",
    "        a2t = a2[t,:].reshape(-1, 1)  # (26, 1)\n",
    "        ht = h[t,:].reshape(-1, 1)  # (10, 1)\n",
    "        yt = y[t,:].reshape(-1, 1)  # (10, 1)\n",
    "        \n",
    "        # Calculamos el error en la capa de salida (delta3), almacenar en d3t\n",
    "        d3t = ht - yt\n",
    "        \n",
    "        # Para calcular el error en la capa oculta (delta2) necesitamos añadir un uno al inicio del vector z2t\n",
    "        # Almacenar en z2t\n",
    "        z2t = np.vstack((1,z2t))\n",
    "        \n",
    "        # Calculamos d2 a partir del error de la capa de salida, los parámetros en theta2 y el gradiente de z2t (guardar en d2t)\n",
    "        # <RELLENAR>\n",
    "        d2t = np.multiply(theta2.T*d3t,sigmoid_gradient(z2t))\n",
    "        \n",
    "        # Ya podemos calcular los gradientes a partir de los errores\n",
    "        # Para calcular el gradiente de los theta1, tenemos en cuenta el error en la capa oculta d2\n",
    "        # Acumular el gradiente del ejemplo en delta1 y delta2\n",
    "        delta1 += d2t[1:]*a1t.T\n",
    "        delta2 += d3t*a2t.T\n",
    "    \n",
    "    # Calculamos el gradiente finalmente dividiendo entre el número de ejemplos\n",
    "    delta1 = delta1/m\n",
    "    delta2 = delta2/m\n",
    "    \n",
    "    # Añadimos el término de regularización en delta1 y delta2 (no regularizar el bias)\n",
    "    \n",
    "    delta1[:,1:] += lambda_reg*theta1[:,1:]/m\n",
    "    delta2[:,1:] += lambda_reg*theta2[:,1:]/m\n",
    "    \n",
    "    # Para pasar los gradientes a minimize los ponemos en un vector\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J, grad = backpropReg(params, input_size, hidden_size, num_labels, X, y_onehot, lambda_reg)\n",
    "print(J, grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red y clasificación\n",
    "\n",
    "Ahora ya estamos listos para entrenar la red y usarla para hacer predicciones. Para entrenarla, utilizamos el método `minimize`de scipy, indicándole que backpropReg es la función que calcula el coste y los gradientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Minimizar la función objetivo que acabamos de definir\n",
    "fmin = minimize(fun=backpropReg, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, lambda_reg), \n",
    "                method='TNC', jac=True, options={'maxiter': 250})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desempaquetamos los parámetros obtenidos como resultado del entrenamiento almacenados en fmin.x\n",
    "# Crea las variables theta1 y theta2\n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "# Utilizamos los parámetros desempaquetados con la propagación hacia adelante para obtener la predicción para nuestros ejemplos\n",
    "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "\n",
    "# Finalmente, para obtener la clase para cada ejemplo, buscamos de las diez salidas cual es la más alta\n",
    "# y usamos su índice como valor predicho (utilizar np.argmax con axis=1 que hace precísamente eso).\n",
    "y_pred = np.argmax(h,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accTrain = metrics.accuracy_score(y_pred, y)\n",
    "\n",
    "print(\"Precisión en train: {}%\".format(accTrain*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = 'FinVelocidadMaxima80'\n",
    "clf = MLPRegressor(hidden_layer_sizes=(64,64,64),activation=\"relu\" ,random_state=1, max_iter=2000)\n",
    "Ytrain_T = transformarY(Ytrain, signal)\n",
    "clf.fit(Xtrain, Ytrain_T.reshape(-1))\n",
    "# clf.predict(Xtrain)\n",
    "clf.predict(Xtest)\n",
    "# clf.score(Xtrain,Ytrain)\n",
    "Ytest_T = transformarY(Ytest, signal)\n",
    "clf.score(Xtest,Ytest_T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
