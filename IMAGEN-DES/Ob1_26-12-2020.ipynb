{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Creaci칩n de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a crear la matriz X, que es la que contiene las im치genes (se crea un dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import skimage\n",
    "\n",
    "\n",
    "#Librerias contornos \n",
    "import numpy as np\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.filters import prewitt_h,prewitt_v\n",
    "from skimage import data, io\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.filters import threshold_otsu \n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris #dataset de prueba\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.optimize import minimize\n",
    "import sklearn.metrics as metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$devolverDiccionarioEtiquetas$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $Sift$\n",
    "#devuelve descriptor de Sift\n",
    "def siftMaker(images00000_mod):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(images00000_mod,None)    \n",
    "    #quizas falta un tercer parametro:  imagen original\n",
    "    img=cv2.drawKeypoints(images00000_mod,kp, images00000_mod, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    #cv2.imwrite('sift_keypoints.jpg',img)\n",
    "    return des\n",
    "\n",
    "\n",
    "# $Surf$\n",
    "def surfmaker(images00000_mod):\n",
    "    surf = cv2.xfeatures2d.SURF_create(400)\n",
    "    kp, des = surf.detectAndCompute(images00000_mod,None)\n",
    "    return des\n",
    "\n",
    "# $Fast$ + $Brief$\n",
    "def fastbrief_Maker(images00000_mod):\n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "    # find and draw the keypoints\n",
    "    kp2 = fast.detect(images00000_mod,None)\n",
    "    img2 = cv2.drawKeypoints(images00000_mod, kp2, None, color=(255,0,0))\n",
    "\n",
    "#     # Print all default params\n",
    "#     print( \"Threshold: {}\".format(fast.getThreshold()) )\n",
    "#     print( \"nonmaxSuppression:{}\".format(fast.getNonmaxSuppression()) )\n",
    "#     print( \"neighborhood: {}\".format(fast.getType()) )\n",
    "#     print( \"Total Keypoints with nonmaxSuppression: {}\".format(len(kp2)) )\n",
    "\n",
    "    # cv2.imwrite('fast_true.png',img2)\n",
    "#     cv2.imshow(\"test1\", img2)\n",
    "\n",
    "    # Disable nonmaxSuppression\n",
    "    fast.setNonmaxSuppression(0)\n",
    "    kp3 = fast.detect(img2,None)\n",
    "\n",
    "#     print( \"Total Keypoints without nonmaxSuppression: {}\".format(len(kp3)) )\n",
    "\n",
    "    img3 = cv2.drawKeypoints(img2, kp3, None, color=(255,0,0))\n",
    "    #cv2.imshow(\"test\", img3)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    # cv2.imwrite('fast_false.png',img3)\n",
    "    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "    # Compute descriptors\n",
    "    kp, des = brief.compute(img3, kp3) #fast nonsup\n",
    "    return des\n",
    "\n",
    "\n",
    "# $ORB$\n",
    "def orbMaker(images00000_mod):\n",
    "    # Initiate ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # find the keypoints with ORB\n",
    "    kp = orb.detect(images00000_mod,None)\n",
    "\n",
    "    # compute the descriptors with ORB\n",
    "    kp, des = orb.compute(images00000_mod, kp)\n",
    "\n",
    "    # draw only keypoints location,not size and orientation\n",
    "    img2 = cv2.drawKeypoints(images00000_mod, kp, None, color=(0,255,0), flags=0)\n",
    "    #plt.imshow(img2), plt.show()\n",
    "    return des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Descripcion: diccionario con los posibles valores de las imagenes, en relacion con el nombre de las carpetas.\n",
    "Entrada: -\n",
    "Salida: diccionario cuyas claves son el nombre de las carpetas, y los valores son el tipo de signals que contiene.\n",
    "'''\n",
    "\n",
    "\n",
    "def devolverDiccionarioEtiquetas():\n",
    "    \n",
    "    diccionarioValores = {}\n",
    "    diccionarioValores['00000'] = 'VelocidadMaxima20'\n",
    "    diccionarioValores['00001'] = 'VelocidadMaxima30'\n",
    "    diccionarioValores['00002'] = 'VelocidadMaxima50'\n",
    "    diccionarioValores['00003'] = 'VelocidadMaxima60'\n",
    "    diccionarioValores['00004'] = 'VelocidadMaxima70'\n",
    "    diccionarioValores['00005'] = 'VelocidadMaxima80'\n",
    "    diccionarioValores['00006'] = 'FinVelocidadMaxima80'\n",
    "    diccionarioValores['00007'] = 'VelocidadMaxima100'\n",
    "    diccionarioValores['00008'] = 'VelocidadMaxima120'\n",
    "    diccionarioValores['00009'] = 'AdelantamientoProhibido'\n",
    "    diccionarioValores['00010'] = 'AdelantamientoProhibidoParaCamiones'\n",
    "\n",
    "    diccionarioValores['00011'] = 'InterseccionConPrioridad'\n",
    "    diccionarioValores['00012'] = 'CalzadaConPrioridad'\n",
    "    diccionarioValores['00013'] = 'CedaElPaso'\n",
    "    diccionarioValores['00014'] = 'DetencionObligatoria'\n",
    "    diccionarioValores['00015'] = 'CirculacionProhibida'\n",
    "    diccionarioValores['00016'] = 'EntradaProhibidaAVehiculosMercancias'\n",
    "    diccionarioValores['00017'] = 'EntradaProhibida'\n",
    "    diccionarioValores['00018'] = 'OtrosPeligros'\n",
    "    diccionarioValores['00019'] = 'CurvaPeligrosaHaciaLaIzquierda'\n",
    "    diccionarioValores['00020'] = 'CurvaPeligrosaHaciaLaDerecha'\n",
    "\n",
    "    diccionarioValores['00021'] = 'CurvasPeligrosasHaciaLaIzquierda'\n",
    "    diccionarioValores['00022'] = 'PerfilIrregular'\n",
    "    diccionarioValores['00023'] = 'PavimentoDeslizante'\n",
    "    diccionarioValores['00024'] = 'EstrechamientoCalzadaPorDerecha'\n",
    "    diccionarioValores['00025'] = 'Obras'\n",
    "    diccionarioValores['00026'] = 'Semaforos'\n",
    "    diccionarioValores['00027'] = 'PasoDePeatones'\n",
    "    diccionarioValores['00028'] = 'Kids'\n",
    "    diccionarioValores['00029'] = 'EntradaProhibidasCiclos'\n",
    "    diccionarioValores['00030'] = 'PavimentoDeslizanteNieveHielo'\n",
    "\n",
    "    diccionarioValores['00031'] = 'PasoDeAnimalesEnLibertad'\n",
    "    diccionarioValores['00032'] = 'FinDeProhibiciones'\n",
    "    diccionarioValores['00033'] = 'SentidoObligatorioDerecha'\n",
    "    diccionarioValores['00034'] = 'SentidoObligatorioIzquierda'\n",
    "    diccionarioValores['00035'] = 'SentidoObligatorioArriba'\n",
    "    diccionarioValores['00036'] = 'DirPermitidasArribaDerecha'\n",
    "    diccionarioValores['00037'] = 'DirPermitidasArribaIzquierda'\n",
    "    diccionarioValores['00038'] = 'PasoObligatorioDerecha'\n",
    "    diccionarioValores['00039'] = 'PasoObligatorioizquierda'\n",
    "    diccionarioValores['00040'] = 'InterseccionSentidoObligatorioGiratorio'\n",
    "\n",
    "\n",
    "    diccionarioValores['00041'] = 'FinProhibicionAdelantamiento'\n",
    "    diccionarioValores['00042'] = 'FinProhibicionAdelantamientoCamiones'\n",
    "    \n",
    "    return diccionarioValores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos ahora el c칩digo...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VelocidadMaxima20\n",
      "VelocidadMaxima30\n",
      "VelocidadMaxima50\n",
      "VelocidadMaxima60\n",
      "VelocidadMaxima70\n",
      "VelocidadMaxima80\n",
      "FinVelocidadMaxima80\n",
      "VelocidadMaxima100\n",
      "VelocidadMaxima120\n",
      "AdelantamientoProhibido\n",
      "AdelantamientoProhibidoParaCamiones\n",
      "InterseccionConPrioridad\n",
      "CalzadaConPrioridad\n",
      "CedaElPaso\n",
      "DetencionObligatoria\n",
      "CirculacionProhibida\n",
      "EntradaProhibidaAVehiculosMercancias\n",
      "EntradaProhibida\n",
      "OtrosPeligros\n",
      "CurvaPeligrosaHaciaLaIzquierda\n",
      "CurvaPeligrosaHaciaLaDerecha\n",
      "CurvasPeligrosasHaciaLaIzquierda\n",
      "PerfilIrregular\n",
      "PavimentoDeslizante\n",
      "EstrechamientoCalzadaPorDerecha\n",
      "Obras\n",
      "Semaforos\n",
      "PasoDePeatones\n",
      "Kids\n",
      "EntradaProhibidasCiclos\n",
      "PavimentoDeslizanteNieveHielo\n",
      "PasoDeAnimalesEnLibertad\n",
      "FinDeProhibiciones\n",
      "SentidoObligatorioDerecha\n",
      "SentidoObligatorioIzquierda\n",
      "SentidoObligatorioArriba\n",
      "DirPermitidasArribaDerecha\n",
      "DirPermitidasArribaIzquierda\n",
      "PasoObligatorioDerecha\n",
      "PasoObligatorioizquierda\n",
      "InterseccionSentidoObligatorioGiratorio\n",
      "FinProhibicionAdelantamiento\n",
      "FinProhibicionAdelantamientoCamiones\n"
     ]
    }
   ],
   "source": [
    "d =  devolverDiccionarioEtiquetas()\n",
    "\n",
    "for k,v in d.items():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearDiccionarioDatos():\n",
    "    trafic_signals = {}\n",
    "    #para cada tipo de se침al creamos una lista con sus ejemplos, y se la asignamos a su etiqueta de nombre de carpeta\n",
    "    for i in range(0,43):\n",
    "        if i<10:\n",
    "            folder = 'Dataset_traffic_sign/0000'+str(i)+'/*.png'\n",
    "            tipo = '0000'+str(i)\n",
    "        else:\n",
    "            folder = 'Dataset_traffic_sign/000'+str(i)+'/*.png'\n",
    "            tipo = '000'+str(i)\n",
    "        \n",
    "        images = [cv2.imread(file,0) for file in glob.glob(folder)]\n",
    "        images_mod = [] \n",
    "        for im in images:\n",
    "            #se aplica el resize\n",
    "            images_mod.append(cv2.resize(im, (100, 100), interpolation = cv2.INTER_NEAREST))\n",
    "\n",
    "        trafic_signals[tipo] = images_mod\n",
    "    return trafic_signals\n",
    "\n",
    "# Creamos nuestra matriz de datos\n",
    "def creaMatrizCaracteristicas(trafic_signals, signals_types, carac_type):\n",
    "    X = 0\n",
    "    Y = 0\n",
    "    \n",
    "    for signal,images in trafic_signals.items():\n",
    "        y = []\n",
    "        x = []\n",
    "        \n",
    "        x = obtenerDesCaracteristicas(np.array(images), carac_type)\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            y.append(signals_types[signal])\n",
    "            \n",
    "        y = np.array(y).reshape(-1,1)\n",
    "        \n",
    "        if signal=='00000':\n",
    "            Y = y\n",
    "            X = x\n",
    "        else:\n",
    "            X = np.vstack((X,x))\n",
    "            Y = np.vstack((Y,y))\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "# Pasamos una np array de imagenes y vamos sacando sus descriptores 1 a 1\n",
    "def obtenerDesCaracteristicas(images_o, carac_type):\n",
    "    des = None\n",
    "    \n",
    "    for i in range(images_o.shape[0]):\n",
    "        if carac_type=='sift':\n",
    "            d = siftMaker(images_o[i])\n",
    "        elif carac_type=='surf':\n",
    "            d = surfmaker(images_o[i])\n",
    "        elif carac_type=='fast_brief':\n",
    "            d = fastbrief_Maker(images_o[i])\n",
    "        elif carac_type=='orb':\n",
    "            d = orbMaker(images_o[i])\n",
    "        else:\n",
    "            try:\n",
    "                raise ValueError\n",
    "            except ValueError:\n",
    "                print(\"valor incorrecto\")\n",
    "                \n",
    "        if d is not None:\n",
    "            if des is None:\n",
    "                des = d\n",
    "            else:\n",
    "                des = np.vstack((des,d))\n",
    "    return des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafic_signals = crearDiccionarioDatos()\n",
    "signals_types = devolverDiccionarioEtiquetas()\n",
    "carac_type = 'orb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = creaMatrizCaracteristicas(trafic_signals, signals_types, carac_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22883, 32), (22883, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 41, 162, 228,  55, 116,  30, 120,  48, 119, 237, 119, 205, 107,\n",
       "        40, 150, 217,  30, 236, 229,  13, 172, 248,  77, 140,  78, 127,\n",
       "       112, 129, 111, 142,  90,  93], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Aplicaci칩n de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci칩n se implementar치n dos modelos de ML como son regresi칩n log칤sitica y redes neuronales para dar soluci칩n al problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi칩n log칤stica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos hemos inclinado por RL en detrimento de SVM pues el n칰mero de caracter칤sticas es bastante superior al n칰mero de ejemplos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformarY():\n",
    "    print(\"Y falta por transformar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'devolverDiccionarioEtiquetas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-708970b4e215>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdiccionario\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdevolverDiccionarioEtiquetas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'devolverDiccionarioEtiquetas' is not defined"
     ]
    }
   ],
   "source": [
    "diccionario  = devolverDiccionarioEtiquetas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIDEA POR ENCIMA....\\n#Y = transformarY(Y)\\nfor k,v in diccionario.items():\\n    Y = nuevaY\\nX, y = load_iris(return_X_y=True)\\nclf = LogisticRegression(random_state=0).fit(X, y)\\nprediccion = clf.predict(X[:2, :])\\nclf.predict_proba(X[:2, :])\\nscore = clf.score(X, y)\\nprint(f\"La precision del clasificador es : {score}\")\\nprint(f\"La prediccion es : {prediccion}\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "IDEA POR ENCIMA....\n",
    "#Y = transformarY(Y)\n",
    "for k,v in diccionario.items():\n",
    "    Y = nuevaY\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "prediccion = clf.predict(X[:2, :])\n",
    "clf.predict_proba(X[:2, :])\n",
    "score = clf.score(X, y)\n",
    "print(f\"La precision del clasificador es : {score}\")\n",
    "print(f\"La prediccion es : {prediccion}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alemt\\anaconda3\\envs\\py37vision\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\alemt\\anaconda3\\envs\\py37vision\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precision del clasificador es : 0.96\n",
      "La prediccion es : [0 0]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "prediccion = clf.predict(X[:2, :])\n",
    "clf.predict_proba(X[:2, :])\n",
    "score = clf.score(X, y)\n",
    "print(f\"La precision del clasificador es : {score}\")\n",
    "print(f\"La prediccion es : {prediccion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "# Aplica el fit_transform de scikit a y para obtener el nuevo y_onehot\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1,1))\n",
    "\n",
    "#esto quiere decir que hay 'n' posibles clases. Se pondra un 1 en la clase que participa, y 0 en la clase que no participa.\n",
    "#La participacion va por  'registros'\n",
    "#Para mas info: https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n",
    "y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, theta1, theta2):\n",
    "    m = X.shape[0]\n",
    "    bias = np.ones(m).reshape(-1,1)\n",
    "    \n",
    "    # A침adimos la columna de unos a X para obtener a1\n",
    "    a1 = np.hstack((bias,X))\n",
    "    \n",
    "    # Calculamos z2 -> ten en cuenta que a1 tiene tantas filas como ejemplos y columnas como atributos + 1\n",
    "    # Por otro lado, theta1 tiene tantas filas como neuronas en la capa oculta y columnas como atributos + 1\n",
    "    z2 = a1*theta1.T\n",
    "    \n",
    "    # A침adimos la columna de unos a la sigmoide de z2 (que es a2) para obtener el a2 definitivo\n",
    "    a2 = np.hstack((np.ones(z2.shape[0]).reshape(-1,1),sigmoid(z2)))\n",
    "    \n",
    "    # Calculamos z3\n",
    "    z3 = a2*theta2.T\n",
    "    \n",
    "    # Obtenemos la salida final en h\n",
    "    h = sigmoid(z3)\n",
    "    \n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(params, input_size, hidden_size, num_labels, X, y):\n",
    "    m = X.shape[0]\n",
    "    # Utilizamos las matrices de numpy por facilidad: * es el producto de matrices y np.multiply elemento por elemento\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # desempaquetamos las matrices con los par치metros para cada capa\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    # Ejecutamos las propagaci칩n hacia adelante para obtener las salidas para cada ejemplo\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # Calculamos el coste\n",
    "    J = (1/m)*(np.sum(np.multiply((-y),np.log(h))-np.multiply((1-y),np.log(1-h))))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 5), (3, 26))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuraci칩n inicial\n",
    "input_size = X.shape[1] #numero de caracteristicas de X\n",
    "hidden_size = 25\n",
    "num_labels = 3\n",
    "np.random.seed(123456789)\n",
    "\n",
    "# Inicializamos los par치metros de la red aleatoriamente\n",
    "# El tama침o del array es el tama침o de las dos matrices de pesos concatenadas\n",
    "params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)) - 0.5) * 0.25\n",
    "\n",
    "# Podemos desempaquetar los par치metros que acabamos de inicializar igual que lo hacemos en la funci칩n de coste\n",
    "theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "# Veamos si los tama침os de las matrices theta1 y theta2 son correctos\n",
    "theta1.shape, theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg = 1.0 #Parametro regularizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costReg(params, input_size, hidden_size, num_labels, X, y, lambda_reg):\n",
    "    m = X.shape[0]\n",
    "    # Utilizamos las matrices de numpy por facilidad: * es el producto de matrices y np.multiply elemento por elemento\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # desempaquetamos las matrices con los par치metros para cada capa, obtener theta1 y theta2\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    # Ejecutamos las propagaci칩n hacia adelante para obtener las salidas para cada ejemplo\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # Calculamos el coste\n",
    "    J = (1/m)*(np.sum(np.multiply((-y),np.log(h))-np.multiply((1-y),np.log(1-h))))\n",
    "    \n",
    "    # Es decir, la suma de los par치metros al cuadrado sin considerar la primera columna en ninguna de las dos matrices de par치metros\n",
    "    J += (lambda_reg/(2*m))*(np.sum(np.sum(np.multiply(theta1,theta1))+np.sum(np.multiply(theta2,theta2))))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    g = sigmoid(z)\n",
    "    return np.multiply(g,(1-g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropReg(params, input_size, hidden_size, num_labels, X, y, lambda_reg):\n",
    "    ###################################################################\n",
    "    # Copiar aqu칤 el c칩digo de la funci칩n de coste con regularizaci칩n #\n",
    "    ###################################################################\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    # Utilizamos las matrices de numpy por facilidad: * es el producto de matrices y np.multiply elemento por elemento\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # desempaquetamos las matrices con los par치metros para cada capa, obtener theta1 y theta2\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    # Ejecutamos las propagaci칩n hacia adelante para obtener las salidas para cada ejemplo\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # Calculamos el coste\n",
    "    J = (1/m)*(np.sum(np.multiply((-y),np.log(h))-np.multiply((1-y),np.log(1-h))))\n",
    "    \n",
    "    # Es decir, la suma de los par치metros al cuadrado sin considerar la primera columna en ninguna de las dos matrices de par치metros\n",
    "    J += (lambda_reg/(2*m))*(np.sum(np.sum(np.multiply(theta1,theta1))+np.sum(np.multiply(theta2,theta2))))\n",
    "    \n",
    "     ############################\n",
    "    # Comienza Backpropagation #\n",
    "    ############################\n",
    "    # Inicializamos los acumuladores delta1  y delta2 a ceros, con las dismensiones de los theta1 y theta2\n",
    "    # tendr치n dimensiones (25, 401) y (10, 26), respectivamente\n",
    "    delta1 = np.zeros(theta1.shape)\n",
    "    delta2 = np.zeros(theta2.shape)\n",
    "    \n",
    "    # Aunque podr칤amos vectorizarlo vamos a hacerlo para cada ejemplo\n",
    "    for t in range(m):\n",
    "        # Obtenemos lo que necesitamos del ejemplo (c치lculos obtenidos en la propagaci칩n hacia adelante)\n",
    "        # Para usar las f칩rmulas tal y como aparecen, vamos a coger todos los vectores en forma columna (resahpe(-1,1))\n",
    "        a1t = a1[t,:].reshape(-1, 1)  # (401, 1)\n",
    "        z2t = z2[t,:].reshape(-1, 1)  # (25, 1)\n",
    "        a2t = a2[t,:].reshape(-1, 1)  # (26, 1)\n",
    "        ht = h[t,:].reshape(-1, 1)  # (10, 1)\n",
    "        yt = y[t,:].reshape(-1, 1)  # (10, 1)\n",
    "        \n",
    "        # Calculamos el error en la capa de salida (delta3), almacenar en d3t\n",
    "        d3t = ht - yt\n",
    "        \n",
    "        # Para calcular el error en la capa oculta (delta2) necesitamos a침adir un uno al inicio del vector z2t\n",
    "        # Almacenar en z2t\n",
    "        z2t = np.vstack((1,z2t))\n",
    "        \n",
    "        # Calculamos d2 a partir del error de la capa de salida, los par치metros en theta2 y el gradiente de z2t (guardar en d2t)\n",
    "        # <RELLENAR>\n",
    "        d2t = np.multiply(theta2.T*d3t,sigmoid_gradient(z2t))\n",
    "        \n",
    "        # Ya podemos calcular los gradientes a partir de los errores\n",
    "        # Para calcular el gradiente de los theta1, tenemos en cuenta el error en la capa oculta d2\n",
    "        # Acumular el gradiente del ejemplo en delta1 y delta2\n",
    "        delta1 += d2t[1:]*a1t.T\n",
    "        delta2 += d3t*a2t.T\n",
    "    \n",
    "    # Calculamos el gradiente finalmente dividiendo entre el n칰mero de ejemplos\n",
    "    delta1 = delta1/m\n",
    "    delta2 = delta2/m\n",
    "    \n",
    "    # A침adimos el t칠rmino de regularizaci칩n en delta1 y delta2 (no regularizar el bias)\n",
    "    \n",
    "    delta1[:,1:] += lambda_reg*theta1[:,1:]/m\n",
    "    delta2[:,1:] += lambda_reg*theta2[:,1:]/m\n",
    "    \n",
    "    # Para pasar los gradientes a minimize los ponemos en un vector\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9785097686071078 (203,)\n"
     ]
    }
   ],
   "source": [
    "J, grad = backpropReg(params, input_size, hidden_size, num_labels, X, y_onehot, lambda_reg)\n",
    "print(J, grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red y clasificaci칩n\n",
    "\n",
    "Ahora ya estamos listos para entrenar la red y usarla para hacer predicciones. Para entrenarla, utilizamos el m칠todo `minimize`de scipy, indic치ndole que backpropReg es la funci칩n que calcula el coste y los gradientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Minimizar la funci칩n objetivo que acabamos de definir\n",
    "fmin = minimize(fun=backpropReg, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, lambda_reg), \n",
    "                method='TNC', jac=True, options={'maxiter': 250})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desempaquetamos los par치metros obtenidos como resultado del entrenamiento almacenados en fmin.x\n",
    "# Crea las variables theta1 y theta2\n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "# Utilizamos los par치metros desempaquetados con la propagaci칩n hacia adelante para obtener la predicci칩n para nuestros ejemplos\n",
    "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "\n",
    "# Finalmente, para obtener la clase para cada ejemplo, buscamos de las diez salidas cual es la m치s alta\n",
    "# y usamos su 칤ndice como valor predicho (utilizar np.argmax con axis=1 que hace prec칤samente eso).\n",
    "y_pred = np.argmax(h,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisi칩n en train: 98.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "accTrain = metrics.accuracy_score(y_pred, y)\n",
    "\n",
    "print(\"Precisi칩n en train: {}%\".format(accTrain*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
